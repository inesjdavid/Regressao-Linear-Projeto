
import numpy as np

class LinearRegression:
    """
    Linear Regression Model with Gradient Descent
    Function: y = wX + b
    """
    
    def __init__(  #parametros do modelo
        
        self,
        learning_rate=0.01,
        n_iterations=1000,     # criterio de paragem 1: nr max de iterações
        tol=None,          # criterio de paragem 2: melhoria min
        min_loss=None      # criterio de paragem 3: erro míniminmo
    ):
        
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.tol = tol
        self.min_loss = min_loss

        self.coef_ = None        #weight
        self.intercept_ = None   #bias
        self.loss_history_ = []  #guarda o erro em cd iteração

        self.n_iter_ = 0         #nr de interações realizadas
        self.stop_reason_ = None #motivo da paragem do treino


    def __repr__(self):
        """
        Showcases the parameters of the Linear Regression Model
        """
        
        trained = self.coef_ is not None
        return (
            f"LinearRegression("f"learning_rate={self.learning_rate}, "f"n_iterations={self.n_iterations}, "f"trained={trained}, "f"n_iter={self.n_iter_})" )


    def _mean_squared_error(self, y_true, y_pred):  #erro
        """
        Function: Mean Squared Error Loss
        """
        
        return np.mean((y_true - y_pred) ** 2)

    def fit(self, X, y): #treino do modelo
        """
        Model Training
        Parameters: 
        X (feature matrix) with shape [n_samples, n_features]
        Y (target values) with shape [n_samples]
        """ 
        
        X = np.array(X, dtype=float)
        y = np.array(y, dtype=float)
        
        #validação dos inputs
        if X.ndim != 2:
            raise ValueError("X must be a 2D matrix with the shape [n_samples, n_features]")

        if y.ndim != 1:
            raise ValueError("Y must be a 1D array of target values.")

        n_samples, n_features = X.shape
        
        #inicialização dos parametros
        self.coef_ = np.zeros(n_features)
        self.intercept_ = 0.0

        #reset do treino
        self.loss_history_.clear()
        self.n_iter_ = 0
        self.stop_reason_ = None

        prev_loss = None

        #gradiente descendente ciclo
        for i in range(self.n_iterations):  
            y_pred = X @ self.coef_ + self.intercept_ #previsão atual

            loss = self._mean_squared_error(y, y_pred) #erro
            self.loss_history_.append(loss)
            self.n_iter_ = i + 1

            # critério de paragem: erro min
            if self.min_loss is not None and loss <= self.min_loss:
                self.stop_reason_ = "Minimum error threshold reached"
                break

            #gradientes
            grad_w = (-2 / n_samples) * (X.T @ (y - y_pred))
            grad_b = (-2 / n_samples) * np.sum(y - y_pred)

            #atualização dos parametros
            self.coef_ -= self.learning_rate * grad_w
            self.intercept_ -= self.learning_rate * grad_b

            #criterio de paragem: melhoria min do erro
            if self.tol is not None and prev_loss is not None:
                if abs(prev_loss - loss) < self.tol:
                    self.stop_reason_ = "Minimum improvement threshold reached"
                    break

            prev_loss = loss

        if self.stop_reason_ is None:
            self.stop_reason_ = "Maximum number of iterations reached"

        return self

    def predict(self, X):
        """
        Prediction of output values with the training model
        """
        if X.ndim != 2:
            raise ValueError("X must be a 2D matrix with the shape [n_samples, n_features]")
        
        X = np.array(X, dtype=float)
        return X @ self.coef_ + self.intercept_

    def score(self, X, y):
        """
        Computes the coefficient of determination R squared
        """

        y = np.array(y, dtype=float)
        y_pred = self.predict(X)

        ss_res = np.sum((y - y_pred) ** 2)
        ss_tot = np.sum((y - np.mean(y)) ** 2)

        return 1 - ss_res / ss_tot